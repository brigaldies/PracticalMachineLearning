---
title: "Machine Learning-Based Qualitative Assessment of a Dumbbell Bicep Curl"
author: "Bertrand Rigaldies"
date: "March 2016"
output: html_document
---

```{r global_options, echo=FALSE, warning=FALSE, message=FALSE, error=TRUE}
# Load the knitr librabry, and set global options silently.
require(knitr)
opts_chunk$set(warning=FALSE, message=FALSE, error=TRUE)
```
```{r fig_and_table_nums, echo=FALSE, results="hide"}
# fig_num is used to number figures in the report.
fig_num <- 1
table_num <- 1
```
```{r libraries, echo=FALSE, results="hide"}
library(knitr) # To process this R markdown file
library(plyr) # Data manipulation
library(dplyr) # More data manipulation
library(xtable) # Nice table presentation
library(ggplot2) # Plotting
library(caret) # Machine learning algorithm
library(rpart) # Machine learning algorithm
library(gbm) # Machine learning algorithm
library(nnet) # Machine learning algorithm
library(randomForest) # Machine learning algorithm
library(foreach) # "For each"" construct
library(doSNOW) # Parallel processing
```

## Abstract

This paper presents a machine learning/predictive model built with the R `caret` package that assesses the quality of the execution of a dumbbell bicep curl. The paper uses a subset of the data collected and generously shared by the researchers in [1]. The best fit model is a Random Forests-based model, that accomplished an estimated 0.99 accuracy, and a 0.45% estimated out-of-sample error rate.

## Data Exploration

### Data Source

The analysis uses a subset of the data collected in [1], and the provided training and testing data sets at available at the following links:

* [training data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)
* [testing data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

### Data Collection

The experiment that produced the data as well as the measured variables and collected observations are described in details in [1]. A brief summary is provided below.

* Six male subjects in the mid-twenties, were each equipped with three Inertial Measurement Unit (IMU) sensors: One on the lumbar belt, one of the arm, and another on the forearm.
* Each subject's dumbbell was also equipped with an IMU.
* Each subject performed ten repetitions of a right-hand unilateral dumbbell bicep curl, in five different ways according to the following classifications:
    + __Class A__: The bicep curl is performed correctly.
    + __Class B__: The bicep curl is not performed correctly by throwing the elbows to the front.
    + __Class C__: The bicep curl is not performed correctly by lifting the dumbbell halfway.
    + __Class D__: The bicep curl is not performed correctly by lowering the dumbbell halfway. 
    + __Class E__: The bicep curl is not performed correctly by throwing the hips to the front.
* Each IMU provided a _"three-axes acceleration, gyroscope, and magnetometer data at a joint sampling of 45 Hz"_ [1].
    
### Data Exploration

#### Data Loading

The training and testing data sets are loaded iN R as shown below (All data and R files are assumed to reside in a single directory.)

```{r data-loading}
# stringAsFactors=FALSE: Do not let R infer what variable is a factor.
# The csv data values of empty strings, "NA", and "#DIV/0!" are loaded are "NA" values.
# Load the original training data set:
training_orig <- read.csv('pml-training.csv', stringsAsFactors = FALSE, strip.white = TRUE, na.strings = c("", "NA", "#DIV/0!"))
training_orig$user_name <- as.factor(training_orig$user_name)
training_orig$classe <- as.factor(training_orig$classe)
# Load the validation data set:
validating <- read.csv('pml-testing.csv', stringsAsFactors = FALSE, strip.white = TRUE, na.strings = c("", "NA", "#DIV/0!"))
validating$user_name <- as.factor(validating$user_name)
```

The provided training data contains __`r dim(training_orig)[1]`__ observations across __`r dim(training_orig)[2] - 1`__ variables (Not including the outcome "classe" variable). The provided testing, or validation, data contains __`r dim(validating)[1]`__ observations across __`r dim(validating)[2]`__ variables (The validating data set does not include the outcome "classe" variable).

#### Data Variables

The data variables can be grouped into four categories:

1. Observation's ID and subject information: `row_num`, and `user_name`.
1. Observation's timing and sliding window (See [1] for details) information: `raw_timestamp_part_1`, `raw_timestamp_part_2`, `cvtd_timestamp`, `new_window`, and `num_window`.
1. IMU's measurements collected from each sensor's accelerometer, gyroscope, and magnetometer, i.e., for the belt's sensor gyroscope (`grep(".*gyros_belt.*", names(training_orig), value = TRUE)`): `gyros_belt_x`, `gyros_belt_y`, and `gyros_belt_z`.
1. Various derived statistics such as min, max, average, variance, etc., for each sliding window (`new_window == 'yes'`), e.g.: `max_roll_belt`, `avg_pitch_forearm`, `var_accel_dumbbell`, etc.

#### "NA" Analysis and Testing Data Variables

The following observations will guide our features extraction in the next section:

* By design of the data collection setup, the derived statistics are calculated at the end each sliding window (`new_window == 'yes'`). The researchers in [1] selected some of the derived statistics for their model's predictors (See section 5.1 in [1] for details). We will note here that the use of derived statistics calculated over the length of a sliding window to build a predictive model is rather intuitive as the model attempts to assess the quality of a complex person's movement over a period of time.
* However, because all records in the provided validating (testing) data do not contain any calculated derived statistics (All "NA" across the corresponding variables), we are forced to identify the relevant predictors among the raw measurements from the sensors (Category #3 variables defined above), and consider the prediction as an exercise executed on an observation-by-observation basis.
    
## Features Extraction

Based on the observations made in the previous section, as as shown in the R code below, we selected our predictors as being the variables in the training data set that have no NA across all observations.

```{r features-extraction}
measurement_vars <- grep(".*belt.*|.*arm.*|.*forearm.*|.*dumbbell.*", names(training_orig), value = TRUE)
measurement_vars_na_count <- data.frame(feature = measurement_vars, non_na_count = sapply(measurement_vars, function(feature) { sum(!is.na(training_orig[,feature]))}))
measurement_non_na_vars <- as.vector(filter(measurement_vars_na_count, non_na_count == dim(training_orig)[1])[, "feature"])
measurement_non_na_vars
training_orig_non_na_vars <- select(training_orig, one_of(append(c("classe"), measurement_non_na_vars)))
```

## Visualization of Some Predictors

The following two plots help visualizing how a particular measurement (predictor) can intuitively help in classifying the quality of a bicep curl. Figure `r fig_num` plots the belt's gyroscope z measurement, and shows that when the subject throws his hips forward (Classe E), the belt's gyroscope z measurements have noticeably more amplitude than the other classes.

```{r gyros_belt_z}
qplot(x = training_orig_non_na_vars$classe, 
      y= training_orig_non_na_vars$gyros_belt_z, 
      fill=training_orig_non_na_vars$classe, 
      geom="boxplot", 
      xlab='Classe', 
      ylab='Predictor: gyros_belt_z', 
      main=paste('Fig.', fig_num, ': Class E Predictor gyros_belt_z by Classe')) + guides(fill=guide_legend(title="Classe"))
```
```{r echo=FALSE, results="hide"}
fig_num <- fig_num + 1
```

Figure `r fig_num` plots the arm's gyroscope y measurement, and shows that when the subject lifts his elbows (Classe B), the arm's gyroscope y measurements have noticeably more amplitude than the other classes. Interestingly, Figure `r fig_num` shows that it is also true for Classe E curls, which is fairly intuitive: Often, throwing the hips forward is also accompanied by a lift of the elbows.

```{r gyros_arm_y}
qplot(x = training_orig_non_na_vars$classe, 
      y= training_orig_non_na_vars$gyros_arm_y, 
      fill=training_orig_non_na_vars$classe, 
      geom="boxplot",
      xlab='Classe', 
      ylab='Predictor: gyros_arm_y', 
      main=paste('Fig.', fig_num, ': Class B Predictor gyros_arm_y by Classe')) + guides(fill=guide_legend(title="Classe"))
```
```{r echo=FALSE, results="hide"}
fig_num <- fig_num + 1
```

The two expected predictors plotted in the previous two figures are only part of a large set of measurements, and we expect the model to draw upon many more to learn how to classify a complex movement executed by a human. The next section describes how we built our model.

## Model Building

### Training Data Partitioning for Cross Validation

Since the provided validating data is used as the "held out data" to validate/score our model, our study design consists in partitioning the provided training data into a training and testing data subsets as shown in the R code below. Each tried model is trained with the training data set, and cross-validated with the testing data set.

```{r data-partitioning}
set.seed(1)
inTrain <- createDataPartition(training_orig_non_na_vars$classe, p = .7, list = FALSE)
training <- training_orig_non_na_vars[inTrain,]
testing <- training_orig_non_na_vars[-inTrain,]
dim(training)
dim(testing)
```

### Pre-Processing: Primary Component Analysis (PCA)

The chosen features, or predictors (See the previous section "Features Extraction") show some correlation, hence a Primary Component Analysis (PCA) is executed, as shown in the R code below:

```{r correlation}
cor_matrix <- abs(cor(training[,-1]))
diag(cor_matrix) <- 0
which(cor_matrix > .9, arr.ind=T)
set.seed(2)
preProc <- preProcess(training[, -1], method="pca")
preProc
```

The PCA reduced the number of variables from `r preProc$dim[2]` to `r preProc$numComp`. However, the next section shows that models without PCA yields higher accuracy rates than those with PCA.

### Model Building and Testing

We tested the following machine learning algorithms:

1. [_glm_](http://topepo.github.io/caret/Generalized_Linear_Model.html): Generalized Linear Model.
1. [_rpart_](http://topepo.github.io/caret/Tree_Based_Model.html): CART Tree-Based Model.
1. [_gbm_](http://topepo.github.io/caret/Boosting.html): Stochastic Gradient Boosting Model.
1. [_nnet_](http://topepo.github.io/caret/Neural_Network.html): Neural Network Model.
1. [_rf_](http://topepo.github.io/caret/Random_Forest.html): Random Forest Model.

In the R code below, each machine learning algorithm is trained and cross-validated with the same `training` and `testing` data sets respectively (See the R code for trainWithMethod helper function in Appendix B). As of trade-off between model's training time and accurary, we used a train control "number" of bootstrap-based resampling iterations of 5 (The default is 25, consult ?trainControl for details.) The train control parameter `returndData` is to FALSE to save on RAM usage.

The care package is [parallel processing framework](http://topepo.github.io/caret/parallel.html)-aware, and the [doSNOW](https://cran.r-project.org/web/packages/doSNOW/index.html) package is used to provide the parallel processing framework with half of the available cores as a trade-off between speed and RAM usage.

Note that the _glm_ algorithm did not work with this data, and therefore is not included in Table 1.

```{r trainWithMethod, echo=FALSE}
trainWithMethod <- function(methodName, 
                            seedNumber,                    
                            trainingData,                             
                            testingData,
                            trControl,
                            doPCA = TRUE) {
    set.seed(seedNumber)
    modelFitPca <- NULL
    cMatrixPca <- NULL
    execTimePca <- NULL
    # ---------- With PCA ----------
    if (doPCA == TRUE) {
        preProc <- preProcess(trainingData[, -1], method="pca")
        trainPC <- predict(preProc, trainingData[,-1])    
        
        message(paste("Training with method", methodName, "with PCA ..."))
        execTimePca <- system.time({       
            try(
                if (methodName == 'gbm') {
                    modelFitPca <- train(trainingData$classe ~ ., method=methodName, data=trainPC, trControl=trControl, verbose=FALSE)
                } else {
                    modelFitPca <- train(trainingData$classe ~ ., method=methodName, data=trainPC, trControl=trControl)
                }
            )
        })
        message(paste("Training with method", methodName, "with PCA completed in", round(execTimePca["elapsed"], 2), "secs"))
        
        if (is.null(modelFitPca)) { 
            message(paste("Training with method", methodName, "with PCA failed."))
        } else {
            testPC <- predict(preProc, testingData[,-1])
            cMatrixPca <- confusionMatrix(testingData$classe, predict(modelFitPca, testPC))
            message(paste("Testing with method", methodName, "with PCA reached an accuracy of", round(cMatrixPca$overall["Accuracy"], 2)))
        }
    } else {
        message(paste("Training with method", methodName, "with PCA skipped."))
    }
    # ---------- Without PCA ----------
    set.seed(seedNumber)
    modelFit <- NULL
    cMatrix <- NULL
    execTime <- NULL
    message(paste("Training with method", methodName, "..."))
    execTime <- system.time({
        try(
            if (methodName == 'gbm') {
                modelFit <- train(classe ~ ., method=methodName, data=trainingData, trControl=trControl, verbose=FALSE)
            } else {
                modelFit <- train(classe ~ ., method=methodName, data=trainingData, trControl=trControl)
            }
        )
    })
    message(paste("Training with method", methodName, "completed in", round(execTime["elapsed"], 2), "secs"))
    
    if (is.null(modelFit)) { 
        message(paste("Training with method", methodName, "failed."))
    } else {    
        cMatrix <- confusionMatrix(testingData$classe, predict(modelFit, testingData))
        message(paste("Testing with method", methodName, "reached an accuracy of", round(cMatrix$overall["Accuracy"], 2)))
    }
    
    # Return the results
    result <- list(
        method=methodName,        
        modelFit = modelFit,
        execTime = execTime["elapsed"],
        cMatrix = cMatrix,
        modelFitPca = modelFitPca,
        execTimePca = execTimePca["elapsed"],
        cMatrixPca = cMatrixPca)    
    return(result)
}
```
```{r parallel-processing-setup}
# Setup the doSNOW parallel processing "backend"
cluster <- makeCluster(detectCores()/2)
registerDoSNOW(cluster)
```
```{r train_and_cross_validate, cache=TRUE}
trControl <- trainControl(number=5, returnData = FALSE)
execTime <- system.time({    
    modelFit.rpart <- trainWithMethod("rpart", 2, training, testing, trControl)
    modelFit.gbm <- trainWithMethod("gbm", 3, training, testing, trControl)
    modelFit.nnet <- trainWithMethod("nnet", 5, training, testing, trControl)
    modelFit.rf <- trainWithMethod("rf", 4, training, testing, trControl)    
})
print(execTime)
```

Table `r table_num` below provides the training and cross-validation results for the selected machine learning algorithms.

```{r training-results}
results <- data.frame(
    method=c('rpart', 'gbm', 'nnet', 'rf'),
    execTime=c(modelFit.rpart$execTime, 
               modelFit.gbm$execTime,
               modelFit.nnet$execTime,
               modelFit.rf$execTime),
    accuracy=c(modelFit.rpart$cMatrix$overall["Accuracy"], 
               modelFit.gbm$cMatrix$overall["Accuracy"],
               modelFit.nnet$cMatrix$overall["Accuracy"],
               modelFit.rf$cMatrix$overall["Accuracy"]), 
    execTimePca=c(modelFit.rpart$execTimePca, 
                  modelFit.gbm$execTimePca,
                  modelFit.nnet$execTimePca,
                  modelFit.rf$execTimePca),
    accuracyPca=c(modelFit.rpart$cMatrixPca$overall["Accuracy"], 
                  modelFit.gbm$cMatrixPca$overall["Accuracy"],
                  modelFit.nnet$cMatrixPca$overall["Accuracy"],
                  modelFit.rf$cMatrixPca$overall["Accuracy"]))
kable(arrange(results, desc(accuracy)), 
      caption=paste0('Table ', table_num, " : Selected Machine Learning Models Training and Cross-Validating Results (Sorted by 'accuracy' descending)"), 
      digits = 4)
```
```{r echo=FALSE, results="hide"}
table_num <- table_num + 1
```

## Model Validation

### Our Best Model

Our best model was built with the _rf_ algorithm. The model's information  is summarized below:

```{r best-model}
# Our best model with 'rf':
modelFit <- modelFit.rf$modelFit
summary(modelFit)
print(modelFit, digits = 4)
modelFit$finalModel
```

### Estimated Out-of-Sample Error Rate

The R code below calculates our model's estimated out-of-sample error rate:

```{r estimated-out-of-sample-error-rate}
testingPred <- predict(modelFit, testing)
modelFit.cm <- confusionMatrix(testing$classe, testingPred)
modelFit.cm
# out of sample error estimate
testingPredRate <- sum(testingPred == testing$classe)/length(testingPred)
```

The estimated out-of-sample error rate is `r sprintf("%0.4f%%", round((1 - testingPredRate) * 100, 4))`.

### Final Model Test

Finally, our best model is tested against the provided test data as shown in the R code below:

```{r final-test}
predict(modelFit, validating)
```

```{r closing, echo=FALSE, results="hide"}
stopCluster(cluster)
```

## References

[1] Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.
[Read more](http://groupware.les.inf.puc-rio.br/har#ixzz43STfOkaC)

## Appendices

### Appendix A: Required R Libraries
```{r required-libraries, eval=FALSE}
library(knitr) # To process this R markdown file
library(plyr) # Data manipulation
library(dplyr) # More data manipulation
library(xtable) # Nice table presentation
library(ggplot2) # Plotting
library(caret) # Machine learning algorithm
library(rpart) # Machine learning algorithm
library(gbm) # Machine learning algorithm
library(nnet) # Machine learning algorithm
library(randomForest) # Machine learning algorithm
library(foreach) # "For each"" construct
library(doSNOW) # Parallel processing
```

### Appendix B: R trainWithMethod
```{r appendix-b, eval=FALSE}
trainWithMethod <- function(methodName, 
                            seedNumber,                    
                            trainingData,                             
                            testingData,
                            trControl,
                            doPCA = TRUE) {
    set.seed(seedNumber)
    modelFitPca <- NULL
    cMatrixPca <- NULL
    execTimePca <- NULL
    # ---------- With PCA ----------
    if (doPCA == TRUE) {
        preProc <- preProcess(trainingData[, -1], method="pca")
        trainPC <- predict(preProc, trainingData[,-1])    
        
        message(paste("Training with method", methodName, "with PCA ..."))
        execTimePca <- system.time({       
            try(
                if (methodName == 'gbm') {
                    modelFitPca <- train(trainingData$classe ~ ., method=methodName, data=trainPC, trControl=trControl, verbose=FALSE)
                } else {
                    modelFitPca <- train(trainingData$classe ~ ., method=methodName, data=trainPC, trControl=trControl)
                }
            )
        })
        message(paste("Training with method", methodName, "with PCA completed in", round(execTimePca["elapsed"], 2), "secs"))
        
        if (is.null(modelFitPca)) { 
            message(paste("Training with method", methodName, "with PCA failed."))
        } else {
            testPC <- predict(preProc, testingData[,-1])
            cMatrixPca <- confusionMatrix(testingData$classe, predict(modelFitPca, testPC))
            message(paste("Testing with method", methodName, "with PCA reached an accuracy of", round(cMatrixPca$overall["Accuracy"], 2)))
        }
    } else {
        message(paste("Training with method", methodName, "with PCA skipped."))
    }
    # ---------- Without PCA ----------
    set.seed(seedNumber)
    modelFit <- NULL
    cMatrix <- NULL
    execTime <- NULL
    message(paste("Training with method", methodName, "..."))
    execTime <- system.time({
        try(
            if (methodName == 'gbm') {
                modelFit <- train(classe ~ ., method=methodName, data=trainingData, trControl=trControl, verbose=FALSE)
            } else {
                modelFit <- train(classe ~ ., method=methodName, data=trainingData, trControl=trControl)
            }
        )
    })
    message(paste("Training with method", methodName, "completed in", round(execTime["elapsed"], 2), "secs"))
    
    if (is.null(modelFit)) { 
        message(paste("Training with method", methodName, "failed."))
    } else {    
        cMatrix <- confusionMatrix(testingData$classe, predict(modelFit, testingData))
        message(paste("Testing with method", methodName, "reached an accuracy of", round(cMatrix$overall["Accuracy"], 2)))
    }
    
    # Return the results
    result <- list(
        method=methodName,        
        modelFit = modelFit,
        execTime = execTime["elapsed"],
        cMatrix = cMatrix,
        modelFitPca = modelFitPca,
        execTimePca = execTimePca["elapsed"],
        cMatrixPca = cMatrixPca)    
    return(result)
}
```

_End of document_